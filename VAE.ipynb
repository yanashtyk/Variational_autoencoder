{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToPILImage\n",
    "to_img = ToPILImage()\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class View (nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(View, self).__init__()\n",
    "        self.size=size\n",
    "        \n",
    "    def forward(self, tensor):\n",
    "        return tensor.view(self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparametrize(mu, logvar):\n",
    "    std=logvar.div(2).exp()\n",
    "    eps=Variable(std.data.new(std.size()).normal_())\n",
    "    return mu+std*eps\n",
    "\n",
    "\n",
    "class BetaVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_dim=10):\n",
    "        super(BetaVAE, self).__init__()\n",
    "        self.z_dim=z_dim\n",
    "        \n",
    "        self.encoder=nn.Sequential(\n",
    "        nn.Conv2d(1, 32, 5, 1),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(32,32, 5, 1),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(32, 32, 4, 2),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(32, 64, 3, 2),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(64, 256, 4, 1),\n",
    "        nn.ReLU(True),\n",
    "        View((-1, 256*1*1)),\n",
    "        nn.Linear(256, z_dim*2))\n",
    "        \n",
    "        self.decoder=nn.Sequential(\n",
    "        nn.Linear(z_dim, 256),\n",
    "        View((-1, 256, 1, 1)),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(256, 64, 4, 1),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(64, 32, 3, 2),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(32, 32, 4, 2),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(32, 32, 5, 1),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(32, 1, 5, 1))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        distributions=self.encoder(x)\n",
    "     \n",
    "        mu=distributions[:, :self.z_dim]\n",
    "        logvar=distributions[:, self.z_dim:]\n",
    "     \n",
    "        z=reparametrize(mu, logvar)\n",
    "        x_recon=self.decoder(z)\n",
    "        \n",
    "        return x_recon, mu, logvar\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_dataset=MNIST(root=\"~/torch_datasets\", train=True, transform=transform, download=True)\n",
    "\n",
    "test_dataset=MNIST(root=\"~/torch_datasets\", train=False, transform=transform, download=True)\n",
    "\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_loader=torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=BetaVAE(32)\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "num_epochs=30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=(train_dataset.targets==1)|(train_dataset.targets==3)|(train_dataset.targets==5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False,  ...,  True, False, False])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data=train_dataset.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.targets=train_dataset.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18294"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ts=(test_dataset.targets==1)|(test_dataset.targets==3)|(test_dataset.targets==5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.data=test_dataset.data[idx_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.targets=test_dataset.targets[idx_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3037"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/30], loss:15.0626\n",
      "epoch [2/30], loss:15.0798\n",
      "epoch [3/30], loss:14.4823\n",
      "epoch [4/30], loss:13.6135\n",
      "epoch [5/30], loss:14.5496\n",
      "epoch [6/30], loss:14.0098\n",
      "epoch [7/30], loss:14.8292\n",
      "epoch [8/30], loss:14.8475\n",
      "epoch [9/30], loss:14.4044\n",
      "epoch [10/30], loss:14.8528\n",
      "epoch [11/30], loss:13.8983\n",
      "epoch [12/30], loss:14.5446\n",
      "epoch [13/30], loss:14.4263\n",
      "epoch [14/30], loss:14.6838\n",
      "epoch [15/30], loss:13.5833\n",
      "epoch [16/30], loss:14.9641\n",
      "epoch [17/30], loss:14.6028\n",
      "epoch [18/30], loss:14.5849\n",
      "epoch [19/30], loss:14.8681\n",
      "epoch [20/30], loss:14.4346\n",
      "epoch [21/30], loss:15.2463\n",
      "epoch [22/30], loss:14.9727\n",
      "epoch [23/30], loss:14.7063\n",
      "epoch [24/30], loss:14.2169\n",
      "epoch [25/30], loss:14.9502\n",
      "epoch [26/30], loss:13.4605\n",
      "epoch [27/30], loss:15.0935\n",
      "epoch [28/30], loss:15.1881\n",
      "epoch [29/30], loss:14.6031\n",
      "epoch [30/30], loss:14.0429\n"
     ]
    }
   ],
   "source": [
    "for epoch in range (num_epochs):\n",
    "    for data in train_loader:\n",
    "        model.train()\n",
    "        inp, _ = data\n",
    "        output, mu, logvar=model(inp)\n",
    "        kl = -0.5 * (1 + logvar - mu.pow(2)- logvar.exp())\n",
    "        kl_loss=kl.sum()/inp.shape[0]\n",
    "        \n",
    "        std=torch.exp(0.5*logvar)\n",
    "        z=torch.empty_like(mu).normal_()*std+mu\n",
    "        \n",
    "        fit=0.5*(output-inp).pow(2)\n",
    "        fit_loss=fit.sum()/inp.shape[0]\n",
    "        \n",
    "        loss= kl_loss + fit_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch+1, num_epochs, loss.item()))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:14.5721\n"
     ]
    }
   ],
   "source": [
    "for data in test_loader:\n",
    "        model.eval()\n",
    "        inp, _ = data\n",
    "        output, mu, logvar=model(inp)\n",
    "        kl = -0.5 * (1 + logvar - mu.pow(2)- logvar.exp())\n",
    "        kl_loss=kl.sum()/inp.shape[0]\n",
    "        \n",
    "        std=torch.exp(0.5*logvar)\n",
    "        z=torch.empty_like(mu).normal_()*std+mu\n",
    "        \n",
    "        fit=0.5*(output-inp).pow(2)\n",
    "        fit_loss=fit.sum()/inp.shape[0]\n",
    "        \n",
    "        loss= kl_loss + fit_loss\n",
    "print('loss:{:.4f}'.format(loss))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './var_autoencoder02.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=test_dataset[89][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAkElEQVR4nGNgGHBw+WczTjnpt/9+IXGZUCSFBFG4qJIMDAxXcEo6MDB8wmmn4b9/j3HqDGVgEMcpKcnAsBynpCMDwxlckqLcDH/P4XTPz39vkfkoOrlYURWjSDoyMOzHZSrDhn9fXXHJCX/6dx+nsaY8aKrRAv4TTkkuBoY1ON1z8N+/QJw6HzAwmODUSS8AACTBHsBB9oLmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1CF35E47988>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_img(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=model(y.reshape((1, -1,28, 28 )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABUElEQVR4nG2S3U4bQQyFvzM7AQohVQOoEkhBqoTEM8Cz8LR9hF73XyhIRaSQQMlmd04vdreZTeorjzy2Px8bMlP+MNumsPYzFwOyQlehl2sMcQCQNttY7F1ev378VqImK+Y4cXh1M739HpKx+kHQ3sWH9KYoLW0CBRidDiirAnmLlnD4PvKUUscS1uN7EI+Od57nQm1qzEZUNX5XzGaVbQFWBuSwOxlVd3NbYFDe0949H1afXxImGfWB6uF5sfi6goC0OaePzorZtEYgOx/FiJPR8vahxVcWNFacvNXdohWglylp52x/+aMkdLp3QeNUH5zuv/4sJdwuK64V4vDEi4fU4OTyIQLjUfl7oWbnbVkByLgYrlazVbS6SoT2cqCIWs4fl3UmS/zHnXz/5fjTfZ2p0t2Qg30wGf+a/qlt1LtgIIFis40tM5BQL6T/f23sL+mJiLpN33nWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x1CF354AD548>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_img(x[0].reshape(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
